{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notepad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be processed using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os       # using operating system dependent functionality (folders)\n",
    "import glob\n",
    "import pandas as pd # data analysis and manipulation\n",
    "import numpy as np    # numerical computing (manipulating and performing operations on arrays of data)\n",
    "import copy     # Can Copy and Deepcopy files so original file is untouched.\n",
    "from ipywidgets import IntSlider, Output\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "#import SimpleITK as sitk\n",
    "import skimage\n",
    "import hashlib\n",
    "import sys\n",
    "sys.path.insert(0, '../') # path to functions\n",
    "from brainspin import file_handler as fh # \n",
    "from brainspin import mold #\n",
    "from brainspin import carve\n",
    "from brainspin.file_handler import Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hash_folder(origin_folder1, file_extension, made, force=False):\n",
    "    \"\"\"Hashing function to be used by command line.\n",
    "\n",
    "    :param origin_folder1: The string of the folder with files to hash\n",
    "    :type origin_folder1: str\n",
    "    :param file_extension: File extension\n",
    "    :type file_extension: str\n",
    "    :param made: file directory where csv with hashes will be put\n",
    "    :type made: str\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(made, 'hash_output.csv')\n",
    "    df = hash_rash(origin_folder1, file_extension)\n",
    "    if not force:\n",
    "        if os.path.isfile(filepath):\n",
    "            return\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(filepath))\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    df.to_csv(filepath)\n",
    "\n",
    "# def save_preprocessed(array, out_fname, force):\n",
    "#     \"\"\"\n",
    "#     This function is written to be called by the cli module.\n",
    "#     It stores arrays in a directory.\n",
    "#     \"\"\"\n",
    "#     if not force:\n",
    "#         if os.path.isfile(out_fname):\n",
    "#             return\n",
    "#     try:\n",
    "#         os.makedirs(os.path.dirname(out_fname))\n",
    "    # except FileExistsError:\n",
    "    #     pass\n",
    "    # np.save(out_fname, array, allow_pickle=False)\n",
    "\n",
    "\n",
    "def hash_rash(origin_folder1, file_extension):\n",
    "    \"\"\"Hashing function to check files are not corrupted or to assure\n",
    "    files are changed.\n",
    "\n",
    "    :param origin_folder1: The string of the folder with files to hash\n",
    "    :type origin_folder1: str\n",
    "    :param file_extension: File extension\n",
    "    :type file_extension: str\n",
    "\n",
    "    :returns: Dataframe with hashes for what is in folder\n",
    "    :rtype: ~pandas.DataFrame\n",
    "    \"\"\"\n",
    "    hash_list = []\n",
    "    file_names = []\n",
    "    files = '**/*.' + file_extension\n",
    "    \n",
    "    non_suspects1 = glob.glob(\n",
    "        os.path.join(origin_folder1, files),\n",
    "        recursive=True,\n",
    "    )\n",
    "    # print(non_suspects1)\n",
    "    BUF_SIZE = 65536\n",
    "    for file in non_suspects1:\n",
    "        sha256 = hashlib.sha256()\n",
    "        with open(file, 'rb') as f:\n",
    "            while True:\n",
    "                data = f.read(BUF_SIZE)\n",
    "                if not data:\n",
    "                    break\n",
    "                sha256.update(data)\n",
    "        result = sha256.hexdigest()\n",
    "        hash_list.append(result)\n",
    "        file_names.append(file)\n",
    "        #print(file_names)\n",
    "    df = pd.DataFrame(hash_list, file_names)\n",
    "    df.columns = [\"hash\"]\n",
    "    df = df.reset_index()\n",
    "    df = df.rename(columns={'index': 'file_name'})\n",
    "    #df.to_csv('out.csv')\n",
    "    print(df)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def return_2DF():\n",
    "\n",
    "    date = pd.date_range('today', periods=20)\n",
    "    DF1 = pd.DataFrame(np.random.rand(20, 2), index=date)\n",
    "\n",
    "    #DF2 = pd.DataFrame(np.random.rand(20, 4), index=date, columns='A B C D'.split())\n",
    "\n",
    "    return DF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-04-02 09:53:31.916088</th>\n",
       "      <td>0.400860</td>\n",
       "      <td>0.503317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-03 09:53:31.916088</th>\n",
       "      <td>0.471951</td>\n",
       "      <td>0.549124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-04 09:53:31.916088</th>\n",
       "      <td>0.198207</td>\n",
       "      <td>0.823448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-05 09:53:31.916088</th>\n",
       "      <td>0.416346</td>\n",
       "      <td>0.654531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-06 09:53:31.916088</th>\n",
       "      <td>0.394839</td>\n",
       "      <td>0.126402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-07 09:53:31.916088</th>\n",
       "      <td>0.805710</td>\n",
       "      <td>0.672907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-08 09:53:31.916088</th>\n",
       "      <td>0.902058</td>\n",
       "      <td>0.201493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-09 09:53:31.916088</th>\n",
       "      <td>0.219033</td>\n",
       "      <td>0.297260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-10 09:53:31.916088</th>\n",
       "      <td>0.709205</td>\n",
       "      <td>0.755190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-11 09:53:31.916088</th>\n",
       "      <td>0.098600</td>\n",
       "      <td>0.734206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-12 09:53:31.916088</th>\n",
       "      <td>0.737123</td>\n",
       "      <td>0.957916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-13 09:53:31.916088</th>\n",
       "      <td>0.557849</td>\n",
       "      <td>0.104375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-14 09:53:31.916088</th>\n",
       "      <td>0.565658</td>\n",
       "      <td>0.613837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-15 09:53:31.916088</th>\n",
       "      <td>0.786139</td>\n",
       "      <td>0.067271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-16 09:53:31.916088</th>\n",
       "      <td>0.930183</td>\n",
       "      <td>0.500064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-17 09:53:31.916088</th>\n",
       "      <td>0.863207</td>\n",
       "      <td>0.768024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-18 09:53:31.916088</th>\n",
       "      <td>0.949911</td>\n",
       "      <td>0.472311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-19 09:53:31.916088</th>\n",
       "      <td>0.515337</td>\n",
       "      <td>0.746952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-20 09:53:31.916088</th>\n",
       "      <td>0.994622</td>\n",
       "      <td>0.249445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-21 09:53:31.916088</th>\n",
       "      <td>0.802729</td>\n",
       "      <td>0.968402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   0         1\n",
       "2023-04-02 09:53:31.916088  0.400860  0.503317\n",
       "2023-04-03 09:53:31.916088  0.471951  0.549124\n",
       "2023-04-04 09:53:31.916088  0.198207  0.823448\n",
       "2023-04-05 09:53:31.916088  0.416346  0.654531\n",
       "2023-04-06 09:53:31.916088  0.394839  0.126402\n",
       "2023-04-07 09:53:31.916088  0.805710  0.672907\n",
       "2023-04-08 09:53:31.916088  0.902058  0.201493\n",
       "2023-04-09 09:53:31.916088  0.219033  0.297260\n",
       "2023-04-10 09:53:31.916088  0.709205  0.755190\n",
       "2023-04-11 09:53:31.916088  0.098600  0.734206\n",
       "2023-04-12 09:53:31.916088  0.737123  0.957916\n",
       "2023-04-13 09:53:31.916088  0.557849  0.104375\n",
       "2023-04-14 09:53:31.916088  0.565658  0.613837\n",
       "2023-04-15 09:53:31.916088  0.786139  0.067271\n",
       "2023-04-16 09:53:31.916088  0.930183  0.500064\n",
       "2023-04-17 09:53:31.916088  0.863207  0.768024\n",
       "2023-04-18 09:53:31.916088  0.949911  0.472311\n",
       "2023-04-19 09:53:31.916088  0.515337  0.746952\n",
       "2023-04-20 09:53:31.916088  0.994622  0.249445\n",
       "2023-04-21 09:53:31.916088  0.802729  0.968402"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF1 = return_2DF()\n",
    "DF1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                    0\n",
      "../not_pushed\\data_anonymized\\EPAD\\040EPAD00007...  1057b8d371d5816349ebef5d56a10fe28332dfccdaae47...\n",
      "../not_pushed\\data_anonymized\\EPAD\\040EPAD00007...  1312dfb271ebad0ad2ccbed73eb26a0d3c4e323efb4a9a...\n",
      "../not_pushed\\data_anonymized\\EPAD\\040EPAD00007...  ef1be36f689e5710ca5c2a7f91251a335c363a42c8d38b...\n",
      "../not_pushed\\data_anonymized\\HCP\\HCA6061757\\T1...  25cc6080428b99883557f28c4ba0c31eac5eafb1bed6c5...\n",
      "../not_pushed\\data_anonymized\\HCP\\HCA6061757\\T2...  ba2b90d8203e5918117906208944e6778e3f1290cbf549...\n",
      "../not_pushed\\data_anonymized\\Insight46\\sub-199...  288278eab96a1b1a8f7a76380d2ab4a32e2cdd93cc685f...\n",
      "../not_pushed\\data_anonymized\\Insight46\\sub-199...  d3f6f6ff76b9f8cd292c4ff14b6e1bdd552baceb976c80...\n",
      "../not_pushed\\data_anonymized\\Insight46\\sub-199...  647db82b345573f4d6123a11aab0033edd61e267607a21...\n",
      "../not_pushed\\data_anonymized\\Insight46\\sub-199...  5a696e998b28cc31eda6b7dab8816e7746bad923f93ccd...\n",
      "../not_pushed\\data_anonymized\\Insight46\\sub-199...  f1531b68cb3c5f8038978ea7a90e62f276e0952b88226a...\n",
      "../not_pushed\\data_anonymized\\Insight46\\sub-199...  8c2c33e1bcb0623df0e1e8f67dec3f2a9e498231d2a968...\n",
      "../not_pushed\\data_anonymized\\SABRE\\17174\\FLAIR...  0b521d1aab53248553b54a67f7710ece5373a590b9b3bb...\n",
      "../not_pushed\\data_anonymized\\SABRE\\17174\\T1_OR...  c5cbf9c26095c7a875f443b01886f0676099c0dd704915...\n",
      "../not_pushed\\data_anonymized\\SABRE\\17174\\ASL_1...  17cb4565676ff788090ab6cb59f40206abca5a6b289a7c...\n",
      "../not_pushed\\data_anonymized\\SABRE\\17174\\ASL_1...  84e952430a3bfe5943309071babc141eaf54a2f1459ebf...\n",
      "../not_pushed\\data_anonymized\\SABRE\\17174\\ASL_1...  1162b602c27dc64b248bc5ced40736e3463a255907a9f3...\n",
      "../not_pushed\\data_anonymized\\SABRE\\17174\\ASL_1...  492d3fb02263e064f8c725c73c811907cf76d6a485ed15...\n",
      "../not_pushed\\data_anonymized\\StrokeMRI\\sub-590...  996627e8e3be007f2a366bf0f818ce038893306125e744...\n",
      "../not_pushed\\data_anonymized\\StrokeMRI\\sub-590...  f6b4d54cb55d33fa5134ff3090ca7c79de6e594d479e60...\n",
      "../not_pushed\\data_anonymized\\StrokeMRI\\sub-590...  fdc8981fb38cdf57722f0d39532a9d84b2e6845ff15051...\n",
      "../not_pushed\\data_anonymized\\StrokeMRI\\sub-590...  f62c00efda5096f077d5b4ab3baeda586dd8fa11fab0cc...\n",
      "../not_pushed\\data_anonymized\\StrokeMRI\\sub-590...  8522df7157d2b6eba539ed75083e3fe261eac6cb1a125e...\n",
      "../not_pushed\\data_anonymized\\StrokeMRI\\sub-590...  8066ce76d1eadf3b8a2f3ec5f56a40b292944dea27c145...\n",
      "../not_pushed\\data_anonymized\\TOP\\sub-0012_1\\FL...  22101b1f2c21d852269ff1abfdcb7b208c7e5d3b9096e6...\n",
      "../not_pushed\\data_anonymized\\TOP\\sub-0012_1\\T1...  c13df60d08d16a65fe3256e3c82d7ebfd82e2915c5b5e1...\n",
      "../not_pushed\\data_anonymized\\TOP\\sub-0012_1\\AS...  f10aaa6c62ca57faf3ac642bae5f6ee7c4d175abfa452f...\n",
      "../not_pushed\\data_anonymized\\TOP\\sub-0012_1\\AS...  762ad6baad2a5b8c095c459178656cd984754671b6f33a...\n",
      "../not_pushed\\data_anonymized\\TOP\\sub-0012_1\\AS...  1d9328b36c42fbda0469f5c0dcd71fbfc1018df1755af0...\n",
      "../not_pushed\\data_anonymized\\TOP\\sub-0012_1\\AS...  ec79b89873605f3c58d00c77af6b401de45005d5af4264...\n"
     ]
    }
   ],
   "source": [
    "fh.hash_folder('../not_pushed', 'gz', 'hope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.read_csv('out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_directory = '../not_pushed'\n",
    "file_directory_list = glob.glob(\n",
    "    os.path.join(file_directory, '**/*.tsv'),\n",
    "    recursive=True,\n",
    "    )\n",
    "for file in file_directory_list:\n",
    "            print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# non_suspects1 = glob.glob(os.path.join('../not_pushed','**/*.tsv' ))\n",
    "# print(non_suspects1)\n",
    "for file in files:\n",
    "        if file.endswith('.tsv'):\n",
    "            print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the config pathways for the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "root_mri_directory = config.get_directory('root_mri_directory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsv_pattern = os.path.join(root_mri_directory, '**/*.tsv')\n",
    "tsv_files = glob.glob(tsv_pattern, recursive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tsv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_example = pd.read_csv(tsv_files[0], sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataframe_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mri_pattern = os.path.join(root_mri_directory, '**/*.gz')\n",
    "gz_files = glob.glob(mri_pattern, recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gz_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# here we can use something not in the base environment just to check that these files exist correctly\n",
    "\n",
    "\n",
    "\n",
    "# A path to an mrid brain .nii image:\n",
    "t1_fn = gz_files[0]\n",
    "\n",
    "# Read the .nii image containing the volume with SimpleITK:\n",
    "sitk_t1 = sitk.ReadImage(t1_fn)\n",
    "\n",
    "# and access the numpy array:\n",
    "t1 = sitk.GetArrayFromImage(sitk_t1)\n",
    "\n",
    "# now display it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly\n",
    "# import plotly.express as px\n",
    "\n",
    "\n",
    "# fig = px.imshow(\n",
    "#     t1,\n",
    "#     facet_col=1,\n",
    "#     animation_frame=0,\n",
    "#     binary_string=True,\n",
    "#     binary_format=\"jpg\",\n",
    "# )\n",
    "# fig.layout.annotations[0][\"text\"] = \"Something0\"\n",
    "# fig.layout.annotations[1][\"text\"] = \"Something2\"\n",
    "# plotly.io.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sitk.Show(sitk_t1, debugOn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "data = t1\n",
    "z, x, y = data.nonzero()\n",
    "ax.scatter(x, y, z, c=z, alpha=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
